import asyncio
import hashlib
import json
import time

from aiohttp import web
from aiohttp.web import Request, Response

import astrbot.api.message_components as Comp
from astrbot.api import AstrBotConfig, logger
from astrbot.api.event import AstrMessageEvent, MessageChain, filter
from astrbot.api.star import Context, Star, register

from .ani_rss_handler import AniRSSHandler
from .media_handler import MediaHandler


@register(
    "media_webhook",
    "Assistant",
    "åª’ä½“é€šçŸ¥ Webhook æ’ä»¶",
    "2.0.0",
    "https://github.com/example/astrbot_plugin_media_webhook",
)
class MediaWebhookPlugin(Star):
    """åª’ä½“é€šçŸ¥ Webhook æ’ä»¶"""

    def __init__(self, context: Context, config: AstrBotConfig):
        super().__init__(context)
        self.config = config

        # æ ¸å¿ƒé…ç½®
        self.webhook_port = config.get("webhook_port", 60071)
        self.group_id = config.get("group_id", "")
        self.platform_name = config.get("platform_name", "aiocqhttp")
        self.batch_min_size = config.get("batch_min_size", 3)
        self.batch_interval_seconds = config.get("batch_interval_seconds", 300)
        self.cache_ttl_seconds = config.get("cache_ttl_seconds", 300)

        # API é…ç½®
        self.tmdb_api_key = config.get("tmdb_api_key", "")
        self.fanart_api_key = config.get("fanart_api_key", "")

        # åˆå§‹åŒ–å­æ¨¡å—
        self.ani_rss_handler = AniRSSHandler()
        self.media_handler = MediaHandler(self.tmdb_api_key, self.fanart_api_key)

        logger.info("åª’ä½“ Webhook æ’ä»¶å­æ¨¡å—åˆå§‹åŒ–å®Œæˆ:")
        logger.info("  - Ani-RSS å¤„ç†å™¨: å·²å¯ç”¨")
        logger.info(
            f"  - åª’ä½“å¤„ç†å™¨: å·²å¯ç”¨ (TMDB: {'æ˜¯' if self.tmdb_api_key else 'å¦'})"
        )

        # æ¶ˆæ¯é˜Ÿåˆ—å’Œç¼“å­˜
        self.message_queue: list[dict] = []
        self.request_cache: dict[str, float] = {}
        self.last_batch_time = time.time()

        # åª’ä½“ç±»å‹æ˜ å°„
        self.media_type_map = {
            "Movie": "ç”µå½±",
            "Series": "å‰§é›†",
            "Season": "å‰§å­£",
            "Episode": "å‰§é›†",
            "Album": "ä¸“è¾‘",
            "Song": "æ­Œæ›²",
            "Video": "è§†é¢‘",
        }

        self.type_emoji_map = {
            "Movie": "ğŸ¬",
            "Series": "ğŸ“º",
            "Season": "ğŸ“º",
            "Episode": "ğŸ“º",
            "Album": "ğŸµ",
            "Song": "ğŸ¶",
            "Video": "ğŸ“¹",
            "Default": "ğŸŒŸ",
        }

        # HTTP æœåŠ¡å™¨
        self.app = None
        self.runner = None
        self.site = None

        # å¯åŠ¨æœåŠ¡
        asyncio.create_task(self.start_webhook_server())
        asyncio.create_task(self.start_batch_processor())

    async def start_webhook_server(self):
        """å¯åŠ¨ Webhook æœåŠ¡å™¨"""
        try:
            self.app = web.Application()
            self.app.router.add_post("/media-webhook", self.handle_webhook)

            self.runner = web.AppRunner(self.app)
            await self.runner.setup()

            self.site = web.TCPSite(self.runner, "0.0.0.0", self.webhook_port)
            await self.site.start()

            logger.info(f"Media Webhook æœåŠ¡å·²å¯åŠ¨ï¼Œç›‘å¬ç«¯å£: {self.webhook_port}")

        except Exception as e:
            logger.error(f"å¯åŠ¨ Webhook æœåŠ¡å¤±è´¥: {e}")

    async def handle_webhook(self, request: Request) -> Response:
        """å¤„ç† Webhook è¯·æ±‚"""
        try:
            # è§£æè¯·æ±‚ä½“
            body_text = await request.text()
            if not body_text:
                return Response(text="è¯·æ±‚ä½“ä¸ºç©º", status=400)

            # è®°å½•è¯·æ±‚ä¿¡æ¯
            headers = dict(request.headers)
            logger.info("æ”¶åˆ° Webhook è¯·æ±‚:")
            logger.info(f"  User-Agent: {headers.get('user-agent', 'N/A')}")
            logger.info(f"  Content-Type: {headers.get('content-type', 'N/A')}")
            logger.info(f"  è¯·æ±‚ä½“é•¿åº¦: {len(body_text)} å­—ç¬¦")

            # é¦–å…ˆæ£€æµ‹æ˜¯å¦ä¸º Ani-RSS æ ¼å¼
            is_ani_rss, ani_rss_data, format_type = (
                self.ani_rss_handler.detect_ani_rss_format(body_text)
            )

            if is_ani_rss:
                logger.info(f"æ£€æµ‹åˆ° Ani-RSS æ•°æ®ï¼Œæ ¼å¼ç±»å‹: {format_type}")

                # å¤„ç† Ani-RSS æ•°æ®
                message_payload = self.ani_rss_handler.process_ani_rss_data(
                    ani_rss_data, format_type
                )

                # éªŒè¯æ¶ˆæ¯è½½è·
                if not self.ani_rss_handler.validate_ani_rss_message(message_payload):
                    logger.error("Ani-RSS æ¶ˆæ¯éªŒè¯å¤±è´¥")
                    return Response(text="Ani-RSS æ¶ˆæ¯æ ¼å¼é”™è¯¯", status=400)

                # æ£€æŸ¥é‡å¤è¯·æ±‚
                if self.is_duplicate_request(ani_rss_data):
                    logger.info("æ£€æµ‹åˆ°é‡å¤çš„ Ani-RSS è¯·æ±‚ï¼Œå¿½ç•¥")
                    return Response(text="é‡å¤è¯·æ±‚", status=200)

                # ç›´æ¥æ·»åŠ åˆ°é˜Ÿåˆ—ï¼ˆä¸è¿›è¡Œ TMDB ä¸°å¯Œï¼‰
                await self.add_ani_rss_to_queue(message_payload)
                return Response(text="Ani-RSS æ¶ˆæ¯å·²åŠ å…¥é˜Ÿåˆ—", status=200)

            # å¤„ç†é Ani-RSS æ•°æ®ï¼ˆåª’ä½“æœåŠ¡å™¨æ•°æ®ï¼‰
            try:
                raw_data = json.loads(body_text)
                logger.info("æˆåŠŸè§£æä¸ºæ ‡å‡† JSON æ ¼å¼")
            except json.JSONDecodeError as e:
                logger.error(f"JSON è§£æå¤±è´¥: {e}")
                return Response(text="æ— æ•ˆçš„ JSON æ ¼å¼", status=400)

            # ä½¿ç”¨åª’ä½“å¤„ç†å™¨å¤„ç†æ•°æ®ï¼ˆè‡ªåŠ¨æ£€æµ‹æ¥æºã€è½¬æ¢æ ¼å¼ã€TMDB ä¸°å¯Œï¼‰
            logger.info("åˆ†å‘åˆ°åª’ä½“å¤„ç†å™¨...")
            media_data = await self.media_handler.process_media_data(
                raw_data, "unknown", headers
            )

            # éªŒè¯å¤„ç†ç»“æœ
            if not self.media_handler.validate_media_data(
                media_data.get("media_data", {})
            ):
                logger.error("åª’ä½“æ•°æ®éªŒè¯å¤±è´¥")
                return Response(text="åª’ä½“æ•°æ®æ ¼å¼é”™è¯¯", status=400)

            # æ£€æŸ¥é‡å¤è¯·æ±‚
            if self.is_duplicate_request(media_data):
                logger.info("æ£€æµ‹åˆ°é‡å¤è¯·æ±‚ï¼Œå¿½ç•¥")
                return Response(text="é‡å¤è¯·æ±‚", status=200)

            # æ·»åŠ åˆ°é˜Ÿåˆ—
            await self.add_to_queue(media_data)
            return Response(text="åª’ä½“æ¶ˆæ¯å·²åŠ å…¥é˜Ÿåˆ—", status=200)

        except Exception as e:
            logger.error(f"Webhook å¤„ç†å‡ºé”™: {e}")
            return Response(text="å¤„ç†æ¶ˆæ¯æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯", status=500)

    def is_duplicate_request(self, media_data: dict) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤è¯·æ±‚ - ä½¿ç”¨å“ˆå¸Œæ ¡éªŒï¼Œæ’é™¤å›¾ç‰‡ä»¥ä¿æŒæ›´é«˜å‡†ç¡®ç‡"""
        request_hash = self.calculate_request_hash(media_data)
        if not request_hash:
            return False

        current_time = time.time()

        # æ¸…ç†è¿‡æœŸç¼“å­˜
        self.cleanup_expired_cache(current_time)

        # æ£€æŸ¥æ˜¯å¦é‡å¤
        if request_hash in self.request_cache:
            cached_time = self.request_cache[request_hash]
            logger.debug(
                f"æ£€æµ‹åˆ°é‡å¤è¯·æ±‚ï¼Œå“ˆå¸Œ: {request_hash[:8]}..., ç¼“å­˜æ—¶é—´: {cached_time}"
            )
            return True

        # ç¼“å­˜æ–°è¯·æ±‚
        self.request_cache[request_hash] = current_time + self.cache_ttl_seconds
        logger.debug(
            f"ç¼“å­˜æ–°è¯·æ±‚ï¼Œå“ˆå¸Œ: {request_hash[:8]}..., è¿‡æœŸæ—¶é—´: {current_time + self.cache_ttl_seconds}"
        )
        return False

    def calculate_request_hash(self, media_data: dict) -> str:
        """è®¡ç®—è¯·æ±‚å“ˆå¸Œå€¼ - æ’é™¤å›¾ç‰‡å’Œä¸ç¨³å®šå­—æ®µä»¥æé«˜å‡†ç¡®ç‡"""
        try:
            # æ ¹æ®æ•°æ®æ¥æºé€‰æ‹©ä¸åŒçš„å“ˆå¸Œç­–ç•¥
            if self.is_ani_rss_data(media_data):
                return self.calculate_ani_rss_hash(media_data)
            return self.calculate_standard_hash(media_data)
        except Exception as e:
            logger.error(f"è®¡ç®—è¯·æ±‚å“ˆå¸Œå¤±è´¥: {e}")
            return ""

    def is_ani_rss_data(self, media_data: dict) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸º Ani-RSS æ•°æ®"""
        return "meassage" in media_data or "text_template" in media_data

    def calculate_ani_rss_hash(self, media_data: dict) -> str:
        """è®¡ç®— Ani-RSS æ•°æ®çš„å“ˆå¸Œå€¼"""
        # å¯¹äº Ani-RSSï¼Œæå–å…³é”®ä¿¡æ¯è¿›è¡Œå“ˆå¸Œ
        if "meassage" in media_data:
            messages = media_data.get("meassage", [])
            text_content = ""
            for msg in messages:
                if isinstance(msg, dict) and msg.get("type") == "text":
                    text_data = msg.get("data", {})
                    text_content = text_data.get("text", "")
                    break

            # ä»æ–‡æœ¬ä¸­æå–å…³é”®ä¿¡æ¯
            hash_data = {
                "content_hash": hashlib.md5(text_content.encode()).hexdigest()[:16],
                "data_type": "ani_rss_message",
            }
        elif "text_template" in media_data:
            template = media_data.get("text_template", "")
            hash_data = {
                "content_hash": hashlib.md5(template.encode()).hexdigest()[:16],
                "data_type": "ani_rss_template",
            }
        else:
            hash_data = {"data_type": "ani_rss_unknown"}

        hash_string = json.dumps(hash_data, sort_keys=True)
        return hashlib.sha256(hash_string.encode()).hexdigest()

    def calculate_standard_hash(self, media_data: dict) -> str:
        """è®¡ç®—æ ‡å‡†åª’ä½“æ•°æ®çš„å“ˆå¸Œå€¼"""
        # æ’é™¤ä¸ç¨³å®šå­—æ®µ
        stable_fields = {
            k: v
            for k, v in media_data.items()
            if k not in ["image_url", "timestamp", "runtime_ticks"]
        }
        hash_string = json.dumps(stable_fields, sort_keys=True)
        return hashlib.sha256(hash_string.encode()).hexdigest()

    def cleanup_expired_cache(self, current_time: float):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        expired_keys = [
            key
            for key, expire_time in self.request_cache.items()
            if current_time > expire_time
        ]
        for key in expired_keys:
            del self.request_cache[key]

        if expired_keys:
            logger.debug(f"æ¸…ç†äº† {len(expired_keys)} ä¸ªè¿‡æœŸç¼“å­˜æ¡ç›®")

    async def add_to_queue(self, message_payload: dict):
        """æ·»åŠ æ¶ˆæ¯è½½è·åˆ°é˜Ÿåˆ—ï¼ˆé€šç”¨æ–¹æ³•ï¼‰"""
        try:
            # æ·»åŠ æ—¶é—´æˆ³ï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
            if "timestamp" not in message_payload:
                message_payload["timestamp"] = time.time()

            # æ·»åŠ åˆ°é˜Ÿåˆ—
            self.message_queue.append(message_payload)

            # è®°å½•æ—¥å¿—
            source = message_payload.get("source", "unknown")
            has_image = bool(message_payload.get("image_url"))
            logger.info(
                f"æ¶ˆæ¯å·²åŠ å…¥é˜Ÿåˆ— [æ¥æº: {source}] {'(å«å›¾ç‰‡)' if has_image else '(æ— å›¾ç‰‡)'}"
            )

        except Exception as e:
            logger.error(f"æ·»åŠ æ¶ˆæ¯åˆ°é˜Ÿåˆ—å¤±è´¥: {e}")

    async def add_ani_rss_to_queue(self, message_payload: dict):
        """æ·»åŠ  Ani-RSS æ¶ˆæ¯åˆ°é˜Ÿåˆ—"""
        try:
            # æ·»åŠ æ—¶é—´æˆ³
            message_payload["timestamp"] = time.time()

            # æ·»åŠ åˆ°é˜Ÿåˆ—
            self.message_queue.append(message_payload)

            # è®°å½•æ—¥å¿—
            has_image = bool(message_payload.get("image_url"))
            format_type = message_payload.get("format_type", "unknown")
            logger.info(
                f"Ani-RSS æ¶ˆæ¯å·²åŠ å…¥é˜Ÿåˆ— [æ ¼å¼: {format_type}] {'(å«å›¾ç‰‡)' if has_image else '(æ— å›¾ç‰‡)'}"
            )

        except Exception as e:
            logger.error(f"æ·»åŠ  Ani-RSS æ¶ˆæ¯åˆ°é˜Ÿåˆ—å¤±è´¥: {e}")

    async def start_batch_processor(self):
        """å¯åŠ¨æ‰¹é‡å¤„ç†å™¨"""
        logger.info("å¯åŠ¨æ‰¹é‡å¤„ç†å™¨")
        while True:
            try:
                await asyncio.sleep(self.batch_interval_seconds)
                await self.process_message_queue()
            except Exception as e:
                logger.error(f"æ‰¹é‡å¤„ç†å™¨å‡ºé”™: {e}")
                await asyncio.sleep(10)

    async def process_message_queue(self):
        """å¤„ç†æ¶ˆæ¯é˜Ÿåˆ—"""
        if not self.message_queue:
            return

        if not self.group_id:
            logger.warning("æœªé…ç½®ç¾¤ç»„IDï¼Œæ— æ³•å‘é€æ¶ˆæ¯")
            return

        messages = self.message_queue.copy()
        self.message_queue.clear()

        logger.info(f"ä»é˜Ÿåˆ—ä¸­å–å‡º {len(messages)} æ¡å¾…å‘æ¶ˆæ¯")

        try:
            # æ ¹æ®æ¶ˆæ¯æ•°é‡å’Œå¹³å°èƒ½åŠ›é€‰æ‹©å‘é€æ–¹å¼
            if (
                len(messages) >= self.batch_min_size
                and self.platform_name.lower() == "aiocqhttp"
            ):
                await self.send_batch_messages(messages)
            else:
                await self.send_individual_messages(messages)

        except Exception as e:
            logger.error(f"å‘é€æ¶ˆæ¯å¤±è´¥: {e}")
        finally:
            self.last_batch_time = time.time()

    async def send_batch_messages(self, messages: list[dict]):
        """å‘é€åˆå¹¶è½¬å‘æ¶ˆæ¯ï¼ˆä»… aiocqhttpï¼‰"""
        group_id = str(self.group_id).replace(":", "_")
        unified_msg_origin = f"{self.platform_name}:GroupMessage:{group_id}"

        logger.info(f"å‘é€åˆå¹¶è½¬å‘: {len(messages)} æ¡æ¶ˆæ¯")

        try:
            # æ„å»ºåˆå¹¶è½¬å‘èŠ‚ç‚¹
            forward_nodes = []
            for msg in messages:
                content_list = []

                # æ·»åŠ å›¾ç‰‡
                if msg.get("image_url"):
                    content_list.append(Comp.Image.fromURL(msg["image_url"]))

                # æ·»åŠ æ–‡æœ¬
                content_list.append(Comp.Plain(msg["message_text"]))

                # åˆ›å»ºè½¬å‘èŠ‚ç‚¹
                node = Comp.Node(
                    uin="2659908767",
                    name="åª’ä½“é€šçŸ¥",
                    content=content_list,  # å¯é…ç½®
                )
                forward_nodes.append(node)

            # åˆ›å»ºåˆå¹¶è½¬å‘æ¶ˆæ¯é“¾
            forward_chain = MessageChain(forward_nodes)
            await self.context.send_message(unified_msg_origin, forward_chain)
            logger.info(f"âœ… æˆåŠŸå‘é€ {len(forward_nodes)} æ¡åˆå¹¶è½¬å‘æ¶ˆæ¯")

        except Exception as e:
            logger.error(f"å‘é€åˆå¹¶è½¬å‘å¤±è´¥: {e}")
            # å›é€€åˆ°å•ç‹¬å‘é€
            await self.send_individual_messages(messages)

    async def send_individual_messages(self, messages: list[dict]):
        """å‘é€å•ç‹¬æ¶ˆæ¯"""
        group_id = str(self.group_id).replace(":", "_")
        unified_msg_origin = f"{self.platform_name}:GroupMessage:{group_id}"

        logger.info(f"å‘é€å•ç‹¬æ¶ˆæ¯: {len(messages)} æ¡æ¶ˆæ¯")

        for i, msg in enumerate(messages, 1):
            try:
                content_list = []

                # æ·»åŠ å›¾ç‰‡
                if msg.get("image_url"):
                    content_list.append(Comp.Image.fromURL(msg["image_url"]))

                # æ·»åŠ æ–‡æœ¬
                content_list.append(Comp.Plain(msg["message_text"]))

                # åˆ›å»ºæ¶ˆæ¯é“¾
                message_chain = MessageChain(content_list)

                await self.context.send_message(unified_msg_origin, message_chain)
                logger.debug(f"âœ… æ¶ˆæ¯ {i}/{len(messages)} å‘é€æˆåŠŸ")

                # æ·»åŠ å»¶è¿Ÿé¿å…é¢‘ç‡é™åˆ¶
                if i < len(messages):
                    await asyncio.sleep(0.5)

            except Exception as e:
                logger.error(f"âŒ æ¶ˆæ¯ {i} å‘é€å¤±è´¥: {e}")

    @filter.command("webhook status")
    async def webhook_status(self, event: AstrMessageEvent):
        """æŸ¥çœ‹ Webhook çŠ¶æ€"""
        queue_size = len(self.message_queue)
        cache_size = len(self.request_cache)

        # è·å–å­æ¨¡å—çŠ¶æ€
        media_stats = self.media_handler.get_processing_stats()

        status_text = f"""ğŸ“Š Media Webhook çŠ¶æ€

ğŸŒ æœåŠ¡çŠ¶æ€: {"è¿è¡Œä¸­" if self.site else "æœªå¯åŠ¨"}
ğŸ“¡ ç›‘å¬ç«¯å£: {self.webhook_port}
ğŸ“‹ é˜Ÿåˆ—æ¶ˆæ¯: {queue_size} æ¡
ğŸ—‚ï¸ ç¼“å­˜æ¡ç›®: {cache_size} æ¡
âš™ï¸ æ‰¹é‡é˜ˆå€¼: {self.batch_min_size} æ¡
â±ï¸ æ‰¹é‡é—´éš”: {self.batch_interval_seconds} ç§’
ğŸ¯ ç›®æ ‡ç¾¤ç»„: {self.group_id or "æœªé…ç½®"}
ğŸ¤– åè®®å¹³å°: {self.platform_name}

ğŸ“‚ å­æ¨¡å—çŠ¶æ€:
  ğŸ¬ åª’ä½“å¤„ç†å™¨: å·²å¯ç”¨
    - TMDB ä¸°å¯Œ: {"å¯ç”¨" if media_stats.get("tmdb_enabled") else "ç¦ç”¨"}
    - æ”¯æŒæ¥æº: {", ".join(media_stats.get("supported_sources", []))}
    - TMDB ç¼“å­˜: {media_stats.get("cache_size", 0)} æ¡
  ğŸ“º Ani-RSS å¤„ç†å™¨: å·²å¯ç”¨"""

        yield event.plain_result(status_text)

    async def terminate(self):
        """æ’ä»¶å¸è½½æ—¶çš„æ¸…ç†å·¥ä½œ"""
        try:
            if self.site:
                await self.site.stop()
            if self.runner:
                await self.runner.cleanup()
            logger.info("Media Webhook æœåŠ¡å·²åœæ­¢")
        except Exception as e:
            logger.error(f"åœæ­¢ Webhook æœåŠ¡æ—¶å‡ºé”™: {e}")
