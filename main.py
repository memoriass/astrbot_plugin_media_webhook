import asyncio
import hashlib
import json
import time

from aiohttp import web
from aiohttp.web import Request, Response

import astrbot.api.message_components as Comp
from astrbot.api import AstrBotConfig, logger
from astrbot.api.event import AstrMessageEvent, MessageChain, filter
from astrbot.api.star import Context, Star, register

from .adapters import AdapterFactory
from .media_handler import MediaHandler
from .processors import AniRSSHandler

# å¸¸é‡å®šä¹‰
DEFAULT_SENDER_ID = "2659908767"
DEFAULT_SENDER_NAME = "åª’ä½“é€šçŸ¥"
DEFAULT_WEBHOOK_PORT = 60071
DEFAULT_BATCH_MIN_SIZE = 3
DEFAULT_CACHE_TTL = 300
DEFAULT_BATCH_INTERVAL = 300


@register(
    "media_webhook",
    "Assistant",
    "åª’ä½“é€šçŸ¥ Webhook æ’ä»¶",
    "2.0.0",
    "https://github.com/example/astrbot_plugin_media_webhook",
)
class MediaWebhookPlugin(Star):
    """åª’ä½“é€šçŸ¥ Webhook æ’ä»¶"""

    def __init__(self, context: Context, config: AstrBotConfig):
        super().__init__(context)
        self.config = config

        # æ ¸å¿ƒé…ç½®
        self.webhook_port = config.get("webhook_port", DEFAULT_WEBHOOK_PORT)
        self.group_id = config.get("group_id", "")
        self.platform_name = config.get("platform_name", "auto")
        self.batch_min_size = config.get("batch_min_size", DEFAULT_BATCH_MIN_SIZE)
        self.batch_interval_seconds = config.get(
            "batch_interval_seconds", DEFAULT_BATCH_INTERVAL
        )
        self.cache_ttl_seconds = config.get("cache_ttl_seconds", DEFAULT_CACHE_TTL)

        # é€‚é…å™¨é…ç½®
        self.sender_id = config.get("sender_id", DEFAULT_SENDER_ID)
        self.sender_name = config.get("sender_name", DEFAULT_SENDER_NAME)

        # API é…ç½®
        self.tmdb_api_key = config.get("tmdb_api_key", "")
        self.fanart_api_key = config.get("fanart_api_key", "")

        # åˆå§‹åŒ–å­æ¨¡å—
        self.ani_rss_handler = AniRSSHandler()
        self.media_handler = MediaHandler(self.tmdb_api_key, self.fanart_api_key)

        # æ‰“å°å·¥ä½œæ­£å¸¸çš„å­æ¨¡å—
        working_modules = []

        # æ£€æŸ¥ Ani-RSS å¤„ç†å™¨
        if self.ani_rss_handler:
            working_modules.append("Ani-RSS å¤„ç†å™¨")

        # æ£€æŸ¥åª’ä½“å¤„ç†å™¨
        if self.media_handler:
            tmdb_status = "TMDB: æ˜¯" if self.tmdb_api_key else "TMDB: å¦"
            working_modules.append(f"åª’ä½“å¤„ç†å™¨ ({tmdb_status})")

        logger.info("åª’ä½“ Webhook æ’ä»¶å­æ¨¡å—åˆå§‹åŒ–å®Œæˆ:")
        for module in working_modules:
            logger.info(f"  âœ… {module}: å·¥ä½œæ­£å¸¸")

        # æ¶ˆæ¯é˜Ÿåˆ—å’Œç¼“å­˜
        self.message_queue: list[dict] = []
        self.request_cache: dict[str, float] = {}
        self.last_batch_time = time.time()

        # åª’ä½“ç±»å‹æ˜ å°„
        self.media_type_map = {
            "Movie": "ç”µå½±",
            "Series": "å‰§é›†",
            "Season": "å‰§å­£",
            "Episode": "å‰§é›†",
            "Album": "ä¸“è¾‘",
            "Song": "æ­Œæ›²",
            "Video": "è§†é¢‘",
        }

        self.type_emoji_map = {
            "Movie": "ğŸ¬",
            "Series": "ğŸ“º",
            "Season": "ğŸ“º",
            "Episode": "ğŸ“º",
            "Album": "ğŸµ",
            "Song": "ğŸ¶",
            "Video": "ğŸ“¹",
            "Default": "ğŸŒŸ",
        }

        # HTTP æœåŠ¡å™¨
        self.app = None
        self.runner = None
        self.site = None

        # å¯åŠ¨æœåŠ¡
        asyncio.create_task(self.start_webhook_server())
        asyncio.create_task(self.start_batch_processor())

    async def start_webhook_server(self):
        """å¯åŠ¨ Webhook æœåŠ¡å™¨"""
        try:
            self.app = web.Application()
            self.app.router.add_post("/media-webhook", self.handle_webhook)

            self.runner = web.AppRunner(self.app)
            await self.runner.setup()

            self.site = web.TCPSite(self.runner, "0.0.0.0", self.webhook_port)
            await self.site.start()

            logger.info(f"Media Webhook æœåŠ¡å·²å¯åŠ¨ï¼Œç›‘å¬ç«¯å£: {self.webhook_port}")

        except Exception as e:
            logger.error(f"å¯åŠ¨ Webhook æœåŠ¡å¤±è´¥: {e}")

    async def handle_webhook(self, request: Request) -> Response:
        """å¤„ç† Webhook è¯·æ±‚"""
        try:
            # è§£æè¯·æ±‚ä½“
            body_text = await request.text()
            if not body_text:
                return Response(text="è¯·æ±‚ä½“ä¸ºç©º", status=400)

            # è®°å½•è¯·æ±‚ä¿¡æ¯
            headers = dict(request.headers)
            logger.info("æ”¶åˆ° Webhook è¯·æ±‚:")
            logger.info(f"  User-Agent: {headers.get('user-agent', 'N/A')}")
            logger.info(f"  Content-Type: {headers.get('content-type', 'N/A')}")
            logger.info(f"  è¯·æ±‚ä½“é•¿åº¦: {len(body_text)} å­—ç¬¦")

            # å°†æ‰€æœ‰æ•°æ®äº¤ç”±æ‰¹é‡å¤„ç†å™¨æ£€æµ‹å’Œå¤„ç†
            await self.add_raw_data_to_queue(body_text, headers)
            return Response(text="æ•°æ®å·²åŠ å…¥é˜Ÿåˆ—", status=200)

        except Exception as e:
            logger.error(f"Webhook å¤„ç†å‡ºé”™: {e}")
            return Response(text="å¤„ç†æ¶ˆæ¯æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯", status=500)

    def is_duplicate_request(self, media_data: dict) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤è¯·æ±‚ - ä½¿ç”¨å“ˆå¸Œæ ¡éªŒï¼Œæ’é™¤å›¾ç‰‡ä»¥ä¿æŒæ›´é«˜å‡†ç¡®ç‡"""
        request_hash = self.calculate_request_hash(media_data)
        if not request_hash:
            return False

        current_time = time.time()

        # æ¸…ç†è¿‡æœŸç¼“å­˜
        self.cleanup_expired_cache(current_time)

        # æ£€æŸ¥æ˜¯å¦é‡å¤
        if request_hash in self.request_cache:
            cached_time = self.request_cache[request_hash]
            logger.debug(
                f"æ£€æµ‹åˆ°é‡å¤è¯·æ±‚ï¼Œå“ˆå¸Œ: {request_hash[:8]}..., ç¼“å­˜æ—¶é—´: {cached_time}"
            )
            return True

        # ç¼“å­˜æ–°è¯·æ±‚
        self.request_cache[request_hash] = current_time + self.cache_ttl_seconds
        logger.debug(
            f"ç¼“å­˜æ–°è¯·æ±‚ï¼Œå“ˆå¸Œ: {request_hash[:8]}..., è¿‡æœŸæ—¶é—´: {current_time + self.cache_ttl_seconds}"
        )
        return False

    def calculate_request_hash(self, media_data: dict) -> str:
        """è®¡ç®—è¯·æ±‚å“ˆå¸Œå€¼ - æ’é™¤å›¾ç‰‡å’Œä¸ç¨³å®šå­—æ®µä»¥æé«˜å‡†ç¡®ç‡"""
        try:
            # æ ¹æ®æ•°æ®æ¥æºé€‰æ‹©ä¸åŒçš„å“ˆå¸Œç­–ç•¥
            if self.is_ani_rss_data(media_data):
                return self.calculate_ani_rss_hash(media_data)
            return self.calculate_standard_hash(media_data)
        except Exception as e:
            logger.error(f"è®¡ç®—è¯·æ±‚å“ˆå¸Œå¤±è´¥: {e}")
            return ""

    def is_ani_rss_data(self, media_data: dict) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸º Ani-RSS æ•°æ®"""
        return "meassage" in media_data or "text_template" in media_data

    def calculate_ani_rss_hash(self, media_data: dict) -> str:
        """è®¡ç®— Ani-RSS æ•°æ®çš„å“ˆå¸Œå€¼"""
        # å¯¹äº Ani-RSSï¼Œæå–å…³é”®ä¿¡æ¯è¿›è¡Œå“ˆå¸Œ
        if "meassage" in media_data:
            messages = media_data.get("meassage", [])
            text_content = ""
            for msg in messages:
                if isinstance(msg, dict) and msg.get("type") == "text":
                    text_data = msg.get("data", {})
                    text_content = text_data.get("text", "")
                    break

            # ä»æ–‡æœ¬ä¸­æå–å…³é”®ä¿¡æ¯
            hash_data = {
                "content_hash": hashlib.md5(text_content.encode()).hexdigest()[:16],
                "data_type": "ani_rss_message",
            }
        elif "text_template" in media_data:
            template = media_data.get("text_template", "")
            hash_data = {
                "content_hash": hashlib.md5(template.encode()).hexdigest()[:16],
                "data_type": "ani_rss_template",
            }
        else:
            hash_data = {"data_type": "ani_rss_unknown"}

        hash_string = json.dumps(hash_data, sort_keys=True)
        return hashlib.sha256(hash_string.encode()).hexdigest()

    def calculate_standard_hash(self, media_data: dict) -> str:
        """è®¡ç®—æ ‡å‡†åª’ä½“æ•°æ®çš„å“ˆå¸Œå€¼"""
        # æ’é™¤ä¸ç¨³å®šå­—æ®µ
        stable_fields = {
            k: v
            for k, v in media_data.items()
            if k not in ["image_url", "timestamp", "runtime_ticks"]
        }
        hash_string = json.dumps(stable_fields, sort_keys=True)
        return hashlib.sha256(hash_string.encode()).hexdigest()

    def cleanup_expired_cache(self, current_time: float):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        expired_keys = [
            key
            for key, expire_time in self.request_cache.items()
            if current_time > expire_time
        ]
        for key in expired_keys:
            del self.request_cache[key]

        if expired_keys:
            logger.debug(f"æ¸…ç†äº† {len(expired_keys)} ä¸ªè¿‡æœŸç¼“å­˜æ¡ç›®")

    async def add_to_queue(self, message_payload: dict):
        """æ·»åŠ æ ‡å‡†åª’ä½“æ¶ˆæ¯åˆ°é˜Ÿåˆ—"""
        try:
            # æ ‡è®°ä¸ºæ ‡å‡†åª’ä½“æ¶ˆæ¯ï¼Œä½¿ç”¨æ‰¹é‡å‘é€é€»è¾‘
            message_payload["message_type"] = "media"

            # æ·»åŠ æ—¶é—´æˆ³ï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
            if "timestamp" not in message_payload:
                message_payload["timestamp"] = time.time()

            # æ·»åŠ åˆ°é˜Ÿåˆ—
            self.message_queue.append(message_payload)

            # è®°å½•æ—¥å¿—
            source = message_payload.get("source", "unknown")
            has_image = bool(message_payload.get("image_url"))
            logger.info(
                f"æ¶ˆæ¯å·²åŠ å…¥é˜Ÿåˆ— [æ¥æº: {source}] {'(å«å›¾ç‰‡)' if has_image else '(æ— å›¾ç‰‡)'}"
            )

        except Exception as e:
            logger.error(f"æ·»åŠ æ¶ˆæ¯åˆ°é˜Ÿåˆ—å¤±è´¥: {e}")

    async def add_raw_data_to_queue(self, body_text: str, headers: dict):
        """æ·»åŠ åŸå§‹æ•°æ®åˆ°é˜Ÿåˆ—ï¼Œç”±æ‰¹é‡å¤„ç†å™¨æ£€æµ‹å’Œå¤„ç†"""
        try:
            # åˆ›å»ºåŸå§‹æ•°æ®è½½è·
            raw_payload = {
                "raw_data": body_text,
                "headers": headers,
                "timestamp": time.time(),
                "message_type": "raw",  # æ ‡è®°ä¸ºåŸå§‹æ•°æ®ï¼Œéœ€è¦æ£€æµ‹
            }

            # æ·»åŠ åˆ°é˜Ÿåˆ—
            self.message_queue.append(raw_payload)

            logger.info("åŸå§‹æ•°æ®å·²åŠ å…¥é˜Ÿåˆ—ï¼Œç­‰å¾…æ‰¹é‡å¤„ç†å™¨æ£€æµ‹")

        except Exception as e:
            logger.error(f"æ·»åŠ åŸå§‹æ•°æ®åˆ°é˜Ÿåˆ—å¤±è´¥: {e}")

    async def add_ani_rss_to_queue(self, message_payload: dict):
        """æ·»åŠ  Ani-RSS æ¶ˆæ¯åˆ°é˜Ÿåˆ—ï¼ˆæ ‡è®°ä¸ºç‹¬ç«‹å‘é€ï¼‰"""
        try:
            # æ ‡è®°ä¸º ani-rss æ¶ˆæ¯ï¼Œä½¿ç”¨ç‹¬ç«‹å‘é€é€»è¾‘
            message_payload["message_type"] = "ani-rss"

            # æ·»åŠ æ—¶é—´æˆ³
            message_payload["timestamp"] = time.time()

            # æ·»åŠ åˆ°é˜Ÿåˆ—
            self.message_queue.append(message_payload)

            # è®°å½•æ—¥å¿—
            has_image = bool(message_payload.get("image_url"))
            format_type = message_payload.get("format_type", "unknown")
            logger.info(
                f"Ani-RSS æ¶ˆæ¯å·²åŠ å…¥é˜Ÿåˆ— [æ ¼å¼: {format_type}] {'(å«å›¾ç‰‡)' if has_image else '(æ— å›¾ç‰‡)'}"
            )

        except Exception as e:
            logger.error(f"æ·»åŠ  Ani-RSS æ¶ˆæ¯åˆ°é˜Ÿåˆ—å¤±è´¥: {e}")

    async def send_ani_rss_message_directly(self, message_payload: dict):
        """ç›´æ¥å‘é€ Ani-RSS æ¶ˆæ¯ï¼ˆç‹¬ç«‹å¤„ç†ï¼Œä¸è¿›å…¥æ‰¹é‡å¤„ç†å™¨ï¼‰"""
        try:
            group_id = str(self.group_id).replace(":", "_")
            unified_msg_origin = (
                f"{self.get_effective_platform_name()}:GroupMessage:{group_id}"
            )

            # è®°å½•æ—¥å¿—
            has_image = bool(message_payload.get("image_url"))
            format_type = message_payload.get("format_type", "unknown")
            logger.info(
                f"ç›´æ¥å‘é€ Ani-RSS æ¶ˆæ¯ [æ ¼å¼: {format_type}] {'(å«å›¾ç‰‡)' if has_image else '(æ— å›¾ç‰‡)'}"
            )

            content_list = []

            # æ·»åŠ å›¾ç‰‡ï¼ˆå¦‚æœæœ‰ï¼‰
            if message_payload.get("image_url"):
                content_list.append(Comp.Image.fromURL(message_payload["image_url"]))

            # æ·»åŠ æ–‡æœ¬
            content_list.append(Comp.Plain(message_payload["message_text"]))

            # åˆ›å»ºæ¶ˆæ¯é“¾
            message_chain = MessageChain(content_list)

            # ç›´æ¥å‘é€æ¶ˆæ¯
            await self.context.send_message(unified_msg_origin, message_chain)
            logger.info("âœ… Ani-RSS æ¶ˆæ¯å‘é€æˆåŠŸ")

        except Exception as e:
            logger.error(f"âŒ Ani-RSS æ¶ˆæ¯å‘é€å¤±è´¥: {e}")
            logger.debug(f"Ani-RSS å‘é€å¤±è´¥è¯¦æƒ…: {e}", exc_info=True)

    async def send_ani_rss_message_individually(self, message_payload: dict):
        """åœ¨æ‰¹é‡å¤„ç†å™¨ä¸­ç‹¬ç«‹å‘é€å•æ¡ Ani-RSS æ¶ˆæ¯"""
        try:
            group_id = str(self.group_id).replace(":", "_")
            unified_msg_origin = (
                f"{self.get_effective_platform_name()}:GroupMessage:{group_id}"
            )

            # è®°å½•æ—¥å¿—
            has_image = bool(message_payload.get("image_url"))
            format_type = message_payload.get("format_type", "unknown")
            logger.debug(
                f"ç‹¬ç«‹å‘é€ Ani-RSS æ¶ˆæ¯ [æ ¼å¼: {format_type}] {'(å«å›¾ç‰‡)' if has_image else '(æ— å›¾ç‰‡)'}"
            )

            content_list = []

            # æ·»åŠ å›¾ç‰‡ï¼ˆå¦‚æœæœ‰ï¼‰
            if message_payload.get("image_url"):
                content_list.append(Comp.Image.fromURL(message_payload["image_url"]))

            # æ·»åŠ æ–‡æœ¬
            content_list.append(Comp.Plain(message_payload["message_text"]))

            # åˆ›å»ºæ¶ˆæ¯é“¾
            message_chain = MessageChain(content_list)

            # å‘é€æ¶ˆæ¯
            await self.context.send_message(unified_msg_origin, message_chain)
            logger.debug("âœ… Ani-RSS æ¶ˆæ¯å‘é€æˆåŠŸ")

        except Exception as e:
            logger.error(f"âŒ Ani-RSS æ¶ˆæ¯å‘é€å¤±è´¥: {e}")
            logger.debug(f"Ani-RSS å‘é€å¤±è´¥è¯¦æƒ…: {e}", exc_info=True)

    async def detect_and_process_raw_data(self, raw_msg: dict) -> dict:
        """æ£€æµ‹å’Œå¤„ç†åŸå§‹æ•°æ®"""
        try:
            body_text = raw_msg.get("raw_data", "")
            headers = raw_msg.get("headers", {})

            # é¦–å…ˆæ£€æµ‹æ˜¯å¦ä¸º Ani-RSS æ ¼å¼
            is_ani_rss, ani_rss_data, format_type = (
                self.ani_rss_handler.detect_ani_rss_format(body_text)
            )

            if is_ani_rss:
                logger.info(f"æ£€æµ‹åˆ° Ani-RSS æ•°æ®ï¼Œæ ¼å¼ç±»å‹: {format_type}")

                # å¤„ç† Ani-RSS æ•°æ®
                message_payload = self.ani_rss_handler.process_ani_rss_data(
                    ani_rss_data, format_type
                )

                # éªŒè¯æ¶ˆæ¯è½½è·
                if not self.ani_rss_handler.validate_ani_rss_message(message_payload):
                    logger.error("Ani-RSS æ¶ˆæ¯éªŒè¯å¤±è´¥")
                    return None

                # æ£€æŸ¥é‡å¤è¯·æ±‚
                if self.is_duplicate_request(ani_rss_data):
                    logger.info("æ£€æµ‹åˆ°é‡å¤çš„ Ani-RSS è¯·æ±‚ï¼Œå¿½ç•¥")
                    return None

                # æ ‡è®°ä¸º ani-rss æ¶ˆæ¯
                message_payload["message_type"] = "ani-rss"
                return message_payload

            # å¤„ç†æ ‡å‡†åª’ä½“æ•°æ®
            try:
                raw_data = json.loads(body_text)
                logger.info("æ£€æµ‹ä¸ºæ ‡å‡†åª’ä½“æ•°æ®")
            except json.JSONDecodeError as e:
                logger.error(f"JSON è§£æå¤±è´¥: {e}")
                return None

            # æ£€æµ‹åª’ä½“æ¥æº
            detected_source = self.media_handler.detect_media_source(raw_data, headers)
            if not detected_source:
                logger.warning("æœªè¯†åˆ«çš„åª’ä½“æ•°æ®æ ¼å¼")
                return None

            logger.info(f"æ£€æµ‹åˆ°åª’ä½“æ¥æº: {detected_source}")

            # ä½¿ç”¨åª’ä½“å¤„ç†å™¨å¤„ç†æ•°æ®
            media_data = await self.media_handler.process_media_data(
                raw_data, detected_source, headers
            )

            # éªŒè¯å¤„ç†ç»“æœ
            if not self.media_handler.validate_media_data(
                media_data.get("media_data", {})
            ):
                logger.error("åª’ä½“æ•°æ®éªŒè¯å¤±è´¥")
                return None

            # æ£€æŸ¥é‡å¤è¯·æ±‚
            if self.is_duplicate_request(media_data):
                logger.info("æ£€æµ‹åˆ°é‡å¤è¯·æ±‚ï¼Œå¿½ç•¥")
                return None

            # æ ‡è®°ä¸ºåª’ä½“æ¶ˆæ¯
            media_data["message_type"] = "media"
            return media_data

        except Exception as e:
            logger.error(f"åŸå§‹æ•°æ®æ£€æµ‹å’Œå¤„ç†å¤±è´¥: {e}")
            return None

    async def send_media_messages_intelligently(self, media_messages: list):
        """æ™ºèƒ½å‘é€æ ‡å‡†åª’ä½“æ¶ˆæ¯ï¼ˆæ ¹æ®åè®®ç«¯é€‰æ‹©æœ€ä¼˜å‘é€æ¨¡å¼ï¼‰"""
        try:
            effective_platform = self.get_effective_platform_name()
            message_count = len(media_messages)

            logger.info(
                f"æ™ºèƒ½å‘é€ {message_count} æ¡åª’ä½“æ¶ˆæ¯ [å¹³å°: {effective_platform}]"
            )

            # æ ¹æ®æ¶ˆæ¯æ•°é‡é€‰æ‹©å‘é€æ¨¡å¼ï¼ˆæ‰€æœ‰åè®®ç«¯ç»Ÿä¸€ä½¿ç”¨ AstrBot pipelineï¼‰
            if message_count >= self.batch_min_size:
                logger.info(f"ä½¿ç”¨ {effective_platform} æ‰¹é‡å‘é€æ¨¡å¼ï¼ˆåˆå¹¶è½¬å‘ï¼‰")
                await self.send_batch_messages(media_messages)
            else:
                logger.info(f"ä½¿ç”¨ {effective_platform} å•ç‹¬å‘é€æ¨¡å¼")
                await self.send_individual_messages(media_messages)

        except Exception as e:
            logger.error(f"æ™ºèƒ½å‘é€åª’ä½“æ¶ˆæ¯å¤±è´¥: {e}")

    async def start_batch_processor(self):
        """å¯åŠ¨æ‰¹é‡å¤„ç†å™¨ï¼ˆæ™ºèƒ½æ£€æµ‹å’Œå‘é€æ‰€æœ‰æ¶ˆæ¯ç±»å‹ï¼‰"""
        logger.info("âœ… æ‰¹é‡å¤„ç†å™¨: å·¥ä½œæ­£å¸¸")
        while True:
            try:
                await asyncio.sleep(self.batch_interval_seconds)
                await self.process_message_queue()
            except Exception as e:
                logger.error(f"æ‰¹é‡å¤„ç†å™¨å‡ºé”™: {e}")
                await asyncio.sleep(10)

    async def process_message_queue(self):
        """å¤„ç†æ¶ˆæ¯é˜Ÿåˆ—ï¼ˆæ ¹æ®æ¶ˆæ¯ç±»å‹ä½¿ç”¨ä¸åŒå‘é€é€»è¾‘ï¼‰"""
        if not self.message_queue:
            return

        if not self.group_id:
            logger.warning("æœªé…ç½®ç¾¤ç»„IDï¼Œæ— æ³•å‘é€æ¶ˆæ¯")
            return

        messages = self.message_queue.copy()
        self.message_queue.clear()

        logger.info(f"ä»é˜Ÿåˆ—ä¸­å–å‡º {len(messages)} æ¡å¾…å‘æ¶ˆæ¯")

        try:
            # åˆ†ç¦»ä¸åŒç±»å‹çš„æ¶ˆæ¯
            raw_data_messages = []
            ani_rss_messages = []
            media_messages = []

            for msg in messages:
                msg_type = msg.get("message_type", "media")
                if msg_type == "raw":
                    raw_data_messages.append(msg)
                elif msg_type == "ani-rss":
                    ani_rss_messages.append(msg)
                else:
                    media_messages.append(msg)

            # å¤„ç†åŸå§‹æ•°æ®ï¼ˆæ£€æµ‹å’Œè½¬æ¢ï¼‰
            if raw_data_messages:
                logger.info(f"æ£€æµ‹å’Œå¤„ç† {len(raw_data_messages)} æ¡åŸå§‹æ•°æ®")
                for raw_msg in raw_data_messages:
                    processed_msg = await self.detect_and_process_raw_data(raw_msg)
                    if processed_msg:
                        # æ ¹æ®æ£€æµ‹ç»“æœåˆ†ç±»
                        if processed_msg.get("message_type") == "ani-rss":
                            ani_rss_messages.append(processed_msg)
                        else:
                            media_messages.append(processed_msg)

            # å¤„ç† Ani-RSS æ¶ˆæ¯ï¼ˆç‹¬ç«‹å‘é€ï¼‰
            if ani_rss_messages:
                logger.info(f"å¤„ç† {len(ani_rss_messages)} æ¡ Ani-RSS æ¶ˆæ¯ï¼ˆç‹¬ç«‹å‘é€ï¼‰")
                for msg in ani_rss_messages:
                    await self.send_ani_rss_message_individually(msg)

            # å¤„ç†æ ‡å‡†åª’ä½“æ¶ˆæ¯ï¼ˆæ™ºèƒ½å‘é€ï¼‰
            if media_messages:
                logger.info(f"å¤„ç† {len(media_messages)} æ¡æ ‡å‡†åª’ä½“æ¶ˆæ¯ï¼ˆæ™ºèƒ½å‘é€ï¼‰")
                await self.send_media_messages_intelligently(media_messages)

        except Exception as e:
            logger.error(f"å‘é€æ¶ˆæ¯å¤±è´¥: {e}")
        finally:
            self.last_batch_time = time.time()

    async def send_batch_messages(self, messages: list[dict]):
        """å‘é€åˆå¹¶è½¬å‘æ¶ˆæ¯ï¼ˆä½¿ç”¨ AstrBot pipelineï¼‰"""
        group_id = str(self.group_id).replace(":", "_")
        unified_msg_origin = (
            f"{self.get_effective_platform_name()}:GroupMessage:{group_id}"
        )

        logger.info(f"å‘é€åˆå¹¶è½¬å‘: {len(messages)} æ¡æ¶ˆæ¯ [ä½¿ç”¨ AstrBot pipeline]")

        try:
            # æ„å»ºè½¬å‘èŠ‚ç‚¹
            nodes = []
            for msg in messages:
                # æ„å»ºå•ä¸ªèŠ‚ç‚¹çš„å†…å®¹
                content_list = []

                # æ·»åŠ å›¾ç‰‡ï¼ˆå¦‚æœæœ‰ï¼‰
                if msg.get("image_url"):
                    image_comp = Comp.Image.fromURL(msg["image_url"])
                    content_list.append(image_comp)

                # æ·»åŠ æ–‡æœ¬
                content_list.append(Comp.Plain(msg["message_text"]))

                # åˆ›å»ºèŠ‚ç‚¹
                node = Comp.Node(
                    uin=self.sender_id,
                    name=self.sender_name,
                    content=content_list,
                )
                nodes.append(node)

            # æ„å»ºæ¶ˆæ¯é“¾
            if len(nodes) == 1:
                # å•ä¸ªèŠ‚ç‚¹ç›´æ¥å‘é€å†…å®¹
                message_chain = MessageChain(nodes[0].content)
            else:
                # å¤šä¸ªèŠ‚ç‚¹ä½¿ç”¨ Nodes ç»„ä»¶
                message_chain = MessageChain([Comp.Nodes(nodes=nodes)])

            # é€šè¿‡ AstrBot pipeline å‘é€æ¶ˆæ¯
            await self.context.send_message(unified_msg_origin, message_chain)
            logger.info("âœ… åˆå¹¶è½¬å‘å‘é€æˆåŠŸ [é€šè¿‡ AstrBot pipeline]")

        except Exception as e:
            logger.error(f"å‘é€åˆå¹¶è½¬å‘å¤±è´¥: {e}")
            logger.debug(f"åˆå¹¶è½¬å‘å¤±è´¥è¯¦æƒ…: {e}", exc_info=True)
            # å›é€€åˆ°å•ç‹¬å‘é€
            logger.info("å›é€€åˆ°å•ç‹¬å‘é€æ¨¡å¼")
            await self.send_individual_messages(messages)

    async def send_individual_messages(self, messages: list[dict]):
        """å‘é€å•ç‹¬æ¶ˆæ¯"""
        group_id = str(self.group_id).replace(":", "_")
        unified_msg_origin = (
            f"{self.get_effective_platform_name()}:GroupMessage:{group_id}"
        )

        logger.info(f"å‘é€å•ç‹¬æ¶ˆæ¯: {len(messages)} æ¡æ¶ˆæ¯")
        logger.info(f"ç›®æ ‡ç¾¤ç»„ID: {group_id}")
        logger.info(f"ç»Ÿä¸€æ¶ˆæ¯æ¥æº: {unified_msg_origin}")

        for i, msg in enumerate(messages, 1):
            try:
                content_list = []

                # æ·»åŠ å›¾ç‰‡
                if msg.get("image_url"):
                    content_list.append(Comp.Image.fromURL(msg["image_url"]))

                # æ·»åŠ æ–‡æœ¬
                content_list.append(Comp.Plain(msg["message_text"]))

                # åˆ›å»ºæ¶ˆæ¯é“¾
                message_chain = MessageChain(content_list)

                logger.info(f"å‡†å¤‡å‘é€æ¶ˆæ¯ {i}: {msg.get('message_text', '')[:50]}...")
                await self.context.send_message(unified_msg_origin, message_chain)
                logger.info(f"âœ… æ¶ˆæ¯ {i}/{len(messages)} å‘é€æˆåŠŸ")

                # æ·»åŠ å»¶è¿Ÿé¿å…é¢‘ç‡é™åˆ¶
                if i < len(messages):
                    await asyncio.sleep(0.5)

            except Exception as e:
                logger.error(f"âŒ æ¶ˆæ¯ {i} å‘é€å¤±è´¥: {e}")
                logger.error(f"é”™è¯¯è¯¦æƒ…: {e}", exc_info=True)

    @filter.command("webhook status")
    async def webhook_status(self, event: AstrMessageEvent):
        """æŸ¥çœ‹ Webhook çŠ¶æ€"""
        queue_size = len(self.message_queue)
        cache_size = len(self.request_cache)

        # è·å–å­æ¨¡å—çŠ¶æ€
        media_stats = self.media_handler.get_processing_stats()

        # è·å–é€‚é…å™¨ä¿¡æ¯
        try:
            adapter = AdapterFactory.create_adapter(self.get_effective_platform_name())
            adapter_info = adapter.get_adapter_info()
            adapter_name = adapter_info.get("name", "Unknown")
            adapter_features = ", ".join(adapter_info.get("features", []))
        except Exception as e:
            adapter_name = f"Error: {str(e)}"
            adapter_features = "N/A"

        status_text = f"""ğŸ“Š Media Webhook çŠ¶æ€

ğŸŒ æœåŠ¡çŠ¶æ€: {"è¿è¡Œä¸­" if self.site else "æœªå¯åŠ¨"}
ğŸ“¡ ç›‘å¬ç«¯å£: {self.webhook_port}
ğŸ“‹ é˜Ÿåˆ—æ¶ˆæ¯: {queue_size} æ¡
ğŸ—‚ï¸ ç¼“å­˜æ¡ç›®: {cache_size} æ¡
âš™ï¸ æ‰¹é‡é˜ˆå€¼: {self.batch_min_size} æ¡
â±ï¸ æ‰¹é‡é—´éš”: {self.batch_interval_seconds} ç§’
ğŸ¯ ç›®æ ‡ç¾¤ç»„: {self.group_id or "æœªé…ç½®"}
ğŸ¤– åè®®å¹³å°: {self.platform_name} {"(è‡ªåŠ¨æ£€æµ‹: " + self.get_effective_platform_name() + ")" if self.platform_name == "auto" else ""}

ğŸ”§ é€‚é…å™¨çŠ¶æ€:
  ğŸ“¡ å½“å‰é€‚é…å™¨: {adapter_name}
  ğŸ›ï¸ é…ç½®ç±»å‹: è‡ªåŠ¨æ¨æ–­
  ğŸ‘¤ å‘é€è€…: {self.sender_name} ({self.sender_id})
  âœ¨ æ”¯æŒåŠŸèƒ½: {adapter_features}

ğŸ“‚ å­æ¨¡å—çŠ¶æ€:
  ğŸ¬ åª’ä½“å¤„ç†å™¨: å·²å¯ç”¨
    - TMDB ä¸°å¯Œ: {"å¯ç”¨" if media_stats.get("tmdb_enabled") else "ç¦ç”¨"}
    - æ”¯æŒæ¥æº: {", ".join(media_stats.get("supported_sources", []))}
    - TMDB ç¼“å­˜: {media_stats.get("cache_size", 0)} æ¡
  ğŸ“º Ani-RSS å¤„ç†å™¨: å·²å¯ç”¨"""

        yield event.plain_result(status_text)

    async def terminate(self):
        """æ’ä»¶å¸è½½æ—¶çš„æ¸…ç†å·¥ä½œ"""
        try:
            if self.site:
                await self.site.stop()
            if self.runner:
                await self.runner.cleanup()
            logger.info("Media Webhook æœåŠ¡å·²åœæ­¢")
        except Exception as e:
            logger.error(f"åœæ­¢ Webhook æœåŠ¡æ—¶å‡ºé”™: {e}")

    def get_available_platforms(self) -> list[dict]:
        """è·å–å½“å‰å¯ç”¨çš„å¹³å°åˆ—è¡¨"""
        platforms = []
        for platform_inst in self.context.platform_manager.platform_insts:
            platform_meta = platform_inst.meta()
            platforms.append(
                {
                    "id": platform_meta.id,
                    "name": platform_meta.name,
                    "description": platform_meta.description,
                }
            )
        return platforms

    def auto_detect_platform(self) -> str:
        """è‡ªåŠ¨æ£€æµ‹æœ€åˆé€‚çš„å¹³å°"""
        available_platforms = self.get_available_platforms()

        if not available_platforms:
            logger.warning("æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨å¹³å°ï¼Œä½¿ç”¨é»˜è®¤å€¼ llonebot")
            return "llonebot"

        # ä¼˜å…ˆçº§é¡ºåºï¼šllonebot > napcat > aiocqhttp > å…¶ä»–
        priority_order = ["llonebot", "napcat", "aiocqhttp"]

        # æŒ‰ä¼˜å…ˆçº§æŸ¥æ‰¾
        for priority_name in priority_order:
            for platform in available_platforms:
                if (
                    priority_name in platform["name"].lower()
                    or priority_name in platform["id"].lower()
                ):
                    logger.info(
                        f"è‡ªåŠ¨æ£€æµ‹åˆ°å¹³å°: {platform['id']} ({platform['name']})"
                    )
                    return platform["id"]

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¼˜å…ˆçº§å¹³å°ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨å¹³å°
        first_platform = available_platforms[0]
        logger.info(
            f"ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨å¹³å°: {first_platform['id']} ({first_platform['name']})"
        )
        return first_platform["id"]

    def get_effective_platform_name(self) -> str:
        """è·å–æœ‰æ•ˆçš„å¹³å°åç§°ï¼ˆå¤„ç†autoæ¨¡å¼ï¼‰"""
        if self.platform_name == "auto":
            return self.auto_detect_platform()
        return self.platform_name
